---
---

@inproceedings{moskvoretskii-etal-2024-taxollama,
    title = {TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks},
    author = {Moskvoretskii, Viktor and
      Neminova, Ekaterina  and
      Lobanova, Alina  and
      Panchenko, Alexander and 
      Nikishina, Irina},
    booktitle = {Proceedings of the 62nd Conference of the Association for Computational Linguistics},
    month = {aug},
    year = {2024},
    address = {Bangkok, Tailand},
    publisher = {Association for Computational Linguistics},
    dimensions = {false},
    bibtex_show= {true},
    selected = {true}
}

@article{moskvoretskii2024low,
  title={Low-Resource Machine Translation through the Lens of Personalized Federated Learning},
  author={Moskvoretskii, Viktor and Tupitsa, Nazarii and Biemann, Chris and Horvath, Samuel and Gorbunov, Eduard and Nikishina, Irina},
  journal={arXiv preprint arXiv:2406.12564},
  year={2024},
  dimensions = {false},
  bibtex_show= {true},
  selected = {true},
  abstract = {We present a new approach based on the Personalized Federated Learning algorithm MeritFed that can be applied to Natural Language Tasks with heterogeneous data. We evaluate it on the Low-Resource Machine Translation task, using the dataset from the Large-Scale Multilingual Machine Translation Shared Task (Small Track #2) and the subset of Sami languages from the multilingual benchmark for Finno-Ugric languages. In addition to its effectiveness, MeritFed is also highly interpretable, as it can be applied to track the impact of each language used for training. Our analysis reveals that target dataset size affects weight distribution across auxiliary languages, that unrelated languages do not interfere with the training, and auxiliary optimizer parameters have minimal impact. Our approach is easy to apply with a few lines of code, and we provide scripts for reproducing the experiments at thishttps://github.com/VityaVitalich/MeritFed}
}


@misc{zhelnin2024giftswgaussiannoiseinjected,
      title={GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs}, 
      author={Maxim Zhelnin and Viktor Moskvoretskii and Egor Shvetsov and Egor Venediktov and Mariya Krylova and Aleksandr Zuev and Evgeny Burnaev},
      year={2024},
      eprint={2408.15300},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.15300}, 
}

@article{moskvoretskii2023imad,
  title={Imad: Image-augmented multi-modal dialogue},
  author={Moskvoretskii, Viktor and Frolov, Anton and Kuznetsov, Denis},
  journal={arXiv preprint arXiv:2305.10512},
  year={2023}
}

@article{moskvoretskii2024self,
  title={Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning},
  author={Moskvoretskii, Viktor and Osin, Dmitry and Shvetsov, Egor and Udovichenko, Igor and Zhelnin, Maxim and Dukhovny, Andrey and Zhimerikina, Anna and Efimov, Albert and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:2401.15935},
  year={2024}
}

@article{andreev2024grapheme,
  title={Grapheme-color synesthesia induction with V4 transcranial direct current stimulation},
  author={Andreev, Sergey and Moskvoretskiy, Viktor and Gorin, Alexey and Zinchenko, Oksana},
  journal={Current Psychology},
  pages={1--6},
  year={2024},
  publisher={Springer}
}

@article{andreev2023induction,
  title={Induction of grapheme-color synesthesia-like effects in non-synesthetes via offline anodal tdcs over visual cortex in area v4},
  author={Andreev, Sergey and Moskvoretsky, Viktor and Gorin, Aleksei and Zinchenko, Oksana},
  journal={Brain Stimulation: Basic, Translational, and Clinical Research in Neuromodulation},
  volume={16},
  number={1},
  pages={274},
  year={2023},
  publisher={Elsevier}
}

@article{osin2024ebes,
  title={EBES: Easy Benchmarking for Event Sequences},
  author={Osin, Dmitry and Udovichenko, Igor and Moskvoretskii, Viktor and Shvetsov, Egor and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:2410.03399},
  year={2024}
}

@inproceedings{moskvoretskii2024large,
  title={Are Large Language Models Good at Lexical Semantics? A Case of Taxonomy Learning},
  author={Moskvoretskii, Viktor and Panchenko, Alexander and Nikishina, Irina},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={1498--1510},
  year={2024}
}
